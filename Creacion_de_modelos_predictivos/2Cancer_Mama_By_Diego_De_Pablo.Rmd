---
title: "2Cancer_Mama_By_Diego_De_Pablo"
subtitle: "Actividad #2. Diseño de modelos predictivos en biomedicina"
author:
- name: "Diego De Pablo (depablodiego@uma.es)"
  affiliation: "Universidade da Málaga"
date: "2024-04-21"
logo: rmarkdown.png
output:
  html_document:
    toc: yes                  # incluir tabla de contenido
    toc_float: no            # toc flotante a la izquierda
    number_sections: yes      # numerar secciones y subsecciones
    code_folding: show        # por defecto el código aparecerá mostrada
    #mathjax: local            # emplea una copia local de MathJax, hay que establecer:
    #self_contained: false     # las dependencias se guardan en ficheros externos
    #lib_dir: libs             # directorio para librerías (Bootstrap, MathJax, ...)
    fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Introducción**

El [cáncer de mama](https://www.mayoclinic.org/es/diseases-conditions/breast-cancer/symptoms-causes/syc-20352470) es un tumor maligno que se origina en el tejido de la glándula mamaria, es el tipo más común de cáncer en mujeres a nivel mundial. Estos tumores malignos se caracterizan por tener un crecimiento descontrolado y la capacidad de extenderse a otros tejidos. Están formados por células que han acumulado alteraciones en su material genético.

La detección temprana del cáncer de mama salva vidas, mejora la calidad de vida y reduce la necesidad de tratamientos invasivos, por eso la creación de modelos predictivos es de gran utilidad. 

## **Caso de estudio**

En este trabajo se estudiará 508 muestras de pacientes recolectadas de varias fuentes, pero siguiendo las mismas pautas, cada muestra está asociada a 8 variables y la variable evento que es el estado de la respuesta patológica ([Metástasis](https://www.cancer.net/es/desplazarse-por-atenci%C3%B3n-del-c%C3%A1ncer/conceptos-b%C3%A1sicos-sobre-el-c%C3%A1ncer/%C2%BFqu%C3%A9-es-la-met%C3%A1stasis)) significa que el cáncer se ha diseminado a una parte del cuerpo distinta de donde comenzó. Cuando esto sucede, los médicos dicen que el cáncer ha hecho “metástasis”.

En esta ocasión se busca **obtener un modelo** para desplegarlo en clínica, es decir, que el oncólogo pueda usar esa ecuación obtenida en la consulta para introducir los valores de un nuevo paciente y obtener un valor entre 0 y 1 que se intepretará como **la probabilidad de que se produzca el evento**, en este caso la recidiva. En base a esta información, el clínico tomará la decisión terapeútica que considere conveniente.

## **Variables del conjunto de datos:**

Si quieres más información de como dicha variable afecta al evento puedes acceder a los links, estos llevarán a
más información: 

*Las variables de nuestro estudio:*

-   **Muestra:** Identificador de la muestra (Es un nombre único que a priori no da información) 
-   **Edad:** valor numérico con la [edad de los pacientes](https://www.cdc.gov/spanish/cancer/breast/basic_info/risk_factors.htm)  que van desde los 24 años hasta los 75 años. (más adelante habrá un apartado donde se profundiza más)
-   **REst:** [Receptores de estrógenos](https://medlineplus.gov/spanish/pruebas-de-laboratorio/pruebas-de-receptores-de-estrogeno-y-de-progesterona/)  (variable binaria con valores N para negativos y P para Positivos)
-   **RPro:** [Receptores de progesterona](https://medlineplus.gov/spanish/pruebas-de-laboratorio/pruebas-de-receptores-de-estrogeno-y-de-progesterona/) (variable binaria con valores N para negativos y P para Positivos)
-   **Her2:** [expresión de HER2](https://www.cancer.org/es/cancer/tipos/cancer-de-seno/comprension-de-un-diagnostico-de-cancer-de-seno/estado-de-her2-del-cancer-de-seno.html) (variable binaria con valores N para normales y P para Sobrexpresado)
-   **Estadio:** [Estadío de enfermedad](https://www.cancer.gov/espanol/cancer/diagnostico-estadificacion/estadificacion) (De T1 a T4)
-   **NodAfec:** [Ganglios afectados](https://thancguide.org/es/cancer-types/neck/metastatic-lymph-nodes/) (de N0 a N3)
-   **Grado:** [grado del tumor](https://www.cancer.gov/espanol/cancer/diagnostico-estadificacion/diagnostico/grado-del-tumor) (de 1 a 3)
-   **Fenotipo:** [subtipo determinado por PAM50](http://www.bio-sequence.com/pam50/) (Es decir: desde Basal, Her2, LumA, LumB y normal)

La variable **evento**:

-   **PCR:** [Estado de la respuesta patológica](https://www.clinicbarcelona.org/asistencia/enfermedades/cancer-de-mama/pruebas-y-diagnostico) (1 para pacientes en metástasis, 0 para los que no)

## **indice explicado:**
A pesar de que este trabajo no existe un guión lineal considero que a la hora de estar avanzado es bueno tener una breve descripción:  

1) *Lectura y corrección de datos:*
La fase inicial del análisis de datos en R se centra en la preparación y limpieza de los datos, abordando aspectos cruciales como la eliminación de datos erróneos, la visualización de datos perdidos, la corrección previa a la imputación, y la imputación de valores faltantes. Se procede con la eliminación de datos faltantes específicamente para el evento principal, como es el caso de los valores no disponibles para la PCR. Además, se llevan a cabo modificaciones en las variables según sea necesario para mejorar la calidad y relevancia de los datos. En particular, se exploran diversas opciones para la agrupación de edades, asegurando así una preparación adecuada para análisis posteriores.

2) *División de datos* 

3) *Entrenamiento de modelos* 

4) *Evaluación de modelo* 


# **Lectura y corrección de datos:**
A continuación se trae el dataset, es un archivo txt así que se lee y se guarda en la variable _datos_cancer_, se muestra algunas filas para ver como vienen los datos inicialmente:
```{r Lectura }
datos_iniciales <- read.table("Datos_Cancer_Mama2.txt", sep = "\t", header = TRUE, stringsAsFactors = TRUE, fileEncoding = "latin1")


head(datos_iniciales, 4) #Imprimo por pantalla las primeras 4 filas para mostrar los datos iniciales
View(datos_iniciales) #Este codigo es útil para poder visualizar los datos en su totalidad antes de modificarlos
```
## **Eliminación de datos erroneos:**
En las variables REst, RPro y Her2, no debería de tener el valor I (solo acepta valores para P (positiva) y para N (negativa)), a continuación se elimina dicha variabley se tomarán como datos perdidos (NA).
```{r Eliminación de datos erroneos}
datos_cancer <-  datos_iniciales
# Reemplazar "I" con NA en cada columna
datos_cancer$REst[datos_cancer$REst == "I"] <- NA
datos_cancer$RPro[datos_cancer$RPro == "I"] <- NA
datos_cancer$Her2[datos_cancer$Her2 == "I"] <- NA

# Eliminar el nivel "I" de los factores
datos_cancer$REst <- droplevels(datos_cancer$REst)
datos_cancer$RPro <- droplevels(datos_cancer$RPro)
datos_cancer$Her2 <- droplevels(datos_cancer$Her2)
#View(datos_cancer)
```
## **Visualización de datos perdidos**

Antes de remplazar los NA podría decirse que es parte fundamental entender el peso de nuestras acciones, cuantos datos perdidos existen en cada variable, si vamos a suponer muchos valores puede afectar la confianza de los resultados, un criterio es aquellas variables cuyos valores NA superen un 10% no sean remplazados.

Viendo la gráfica a continuación podrá observar que podremos seguir trabajando con los datos, la variable con un mayor número de NA es **grado** correspondiendo a cerca de un 7% de todos sus datos 37 son NA que deberemos rellenar con la moda. Igualmente, los datos son muy consistentes. Más adelante se hablará más sobre la variable evento.
```{r grafica_porcentaje_NA}
# Calcula el porcentaje de valores faltantes
grafica_porcentajes_NA <-  function(datos){
  na_percentage <- colMeans(is.na(datos)) * 100

# Convierte los resultados a un data frame para ggplot
na_df <- data.frame(variable = names(na_percentage), percentage = na_percentage)

# Carga la biblioteca ggplot2
library(ggplot2)

# Crea el gráfico de barras
ggplot(na_df, aes(x = variable, y = percentage)) +
  geom_bar(stat = "identity", fill = "red") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Variable", y = "Porcentaje de NA", title = "Porcentaje de valores faltantes por variable")
}

#summary(datos_cancer)
grafica_porcentajes_NA(datos_cancer)


```

## **Arreglo de datos previo a la imputación:**

Como a la hora de imputar o analizar datos usaré funciones que trabajan de manera distinta dependiendo si es un factor, un número, etc. Es importante definir bien el tipo de dato antes de usarlo, la variable grado a pesar de tener valores de 1 a 3 no es exactamente una variable númerica, ya que se categoriza entre el grado 1, 2 o 3. no tiene sentido imputar sus valores perdidos por la media.

Además se va a trabajar en simplificar los datos como puede ser la edad que viene dado en números decimales, no tiene sentido para este trabajo usar valores decimales. Por eso se redondea dejando el dato en enteros.
```{r arreglo_de_Decimales}
#Funcion útil para evitar errores por valores 0,5 en luegar de 0.5
arregla_decimal <- function(columna_decimal) {
  columna_decimal <- gsub(",", ".", columna_decimal)
  return(columna_decimal)
}
datos_cancer$Edad <- arregla_decimal(datos_cancer$Edad)
datos_cancer$Edad <- as.integer(datos_cancer$Edad) #Redondea para abajo usualmente

datos_cancer$Grado <- as.factor(datos_cancer$Grado)
#View(datos_cancer)
```

## **Eliminación de datos NA para el evento (PCR):**

De nuestros *508* datos iniciales, 20 tienen NA en nuestra variable evento, como contamos con datos suficientes podremos eliminar el riesgo de arruinar el modelo predictivo al remplazar esos 20 datos con la moda, es por esto por lo que se eliminarán directamente esas 20 filas, que nos elimina 2 casos de NA en la variable grado. 

Ahora contamos con *488 datos de estudio* pero contamos con una buena fiabilidad de los datos. 
```{r eliminacion NA_evento}
summary(datos_cancer)
#Eliminar las filas con NA en PCR es lo mejor, porque es muy peligroso generar resultados por imputacion en las casillas de la variable evento.
datos_cancer <- datos_cancer[!is.na(datos_cancer$PCR), ]
print("Resumen de datos sin NA en nuestra variable evento")
summary(datos_cancer)

```
## **Imputación de datos perdidos:**

Anteriormente terminamos de manejar los datos perdidos o erróneos, ya habiéndolos catalogados es necesario estimar los valores ausentes en base a los valores válidos de otras variables y/o casos de la muestra. Esto al ser un paso muy común en minería de datos se creara dentro de una función con intención de futuras reutilizaciones. Se logrará estimar valores a través del uso de *moda* en el caso de variables categóricas mientras que para las variables numéricas se usará la *media*. 
```{r Rellenado de datos vacios o NA}
rellenado_NA <- function(dataset) {

  # Convertir los valores nulos "" en NA
  dataset[dataset == ""] <- NA
  
  # Identificar las columnas (2) con valores ausentes (anyNA)
  columnas_ausentes <- colnames(dataset)[apply(dataset, 2, anyNA)]

  # Imputar valores faltantes por la media para las variables numéricas y la moda para las variables categóricas
  dataset[columnas_ausentes] <- lapply(dataset[columnas_ausentes], function(col) {
    if (is.numeric(col)) {
      media <- mean(col, na.rm = TRUE)  # Calcular la media de la columna
      col[is.na(col)] <- media  # Reemplazar los NA con la media
    } else {
      # Convertir las columnas a factores si no lo son
      if (!is.factor(col)) {
        col <- as.factor(col)
      }
      # Calcular la moda de la columna y reemplazar los NA con la moda
      moda <- names(which.max(table(col)))
      col[is.na(col)] <- moda
    }
    return(col)
  })

  # verificación
  if (anyNA(dataset)) {
    warning("Hubo un error inesperado, por algún motivo no sé relleno correctamente")
  }
  return(dataset)
}


datos_cancer <- rellenado_NA(datos_cancer)
#View(datos_cancer)

```

## **Modificación de variables:**
Se hán realizado las siguientes modificaciones tomando en cuenta lo hablado en clase:

-   categorizar las variables (grado podríamos cambiar el 1, 2, 3 por I, II, III).
-   En estadio contamos solo 3 individuos con t0, estos son muy pocos. Tenemos 3 opciones, eliminar las filas, implementarla en t0 o lo que haré que es unir los t0 y t1 y crear una nueva variable llamada t0-t1.
```{r}
# Convertir la columna Grado a tipo carácter
datos_cancer$Grado <- as.character(datos_cancer$Grado)

# Realizar los reemplazos
datos_cancer$Grado <- replace(datos_cancer$Grado, datos_cancer$Grado == "1", "I (1)")
datos_cancer$Grado <- replace(datos_cancer$Grado, datos_cancer$Grado == "2", "II (2)")
datos_cancer$Grado <- replace(datos_cancer$Grado, datos_cancer$Grado == "3", "III (3)")

# Convertir la columna Grado de nuevo a factor, si lo deseas
datos_cancer$Grado <- as.factor(datos_cancer$Grado)

# Contar el número de filas con "T0" en la columna Estadio
num_filas_T0 <- sum(datos_cancer$Estadio == "T0")


# Contar el número de filas con "T0" en la columna Estadio
num_filas_T1 <- sum(datos_cancer$Estadio == "T1")

# Convertir la columna Estadio a tipo carácter
datos_cancer$Estadio <- as.character(datos_cancer$Estadio)

# Realizar los reemplazos
datos_cancer$Estadio[datos_cancer$Estadio == "T0"] <- "T0-T1"
datos_cancer$Estadio[datos_cancer$Estadio == "T1"] <- "T0-T1"

# Contar el número de filas con "T0" en la columna Estadio
num_filas <- sum(datos_cancer$Estadio == "T0-T1")

# Convertir la columna Estadio de nuevo a factor, si lo deseas
datos_cancer$Estadio <- as.factor(datos_cancer$Estadio)

print(paste("Para la variable Estadio el número de filas con T0 es:", num_filas_T0, " y el número de filas con T1 es:", num_filas_T1, "Por lo cual para evitar tener una categoría tan pequeña se unificaran en T0-T1", num_filas))

```
## **Distintas posibilidades de agrupación de edad:**
Se ha encontrado en diversos estudios que el riesgo de desarrollar cáncer de mama aumenta con la edad . En la mayoría de los casos, la enfermedad se desarrolla en mujeres de más de 50 años. De hecho, la mediana de edad para desarrollar cáncer de mama es de 63 años.
En clase se habló sobre la decisión de si debes o no agrupar las edades dependerá de tu conocimiento del dominio y de los resultados de tu análisis exploratorio de datos. Como la materia me está interesando bastante, no obstante sigo con mi falta de experiencia  he decidido probar las siguientes agrupaciones y ver cuál produce un modelo más preciso y robusto.

-   **No agrupar las edades:** Esta decición nos ahorra manipulación de datos, el problema con esto es que las edades van desde los 24 a los 75 años, es decir 49 edades posibles lo cual termina siendo una variable con una amplia posibilidad de valores, a veces dificultando el uso de algunas herramientas o aumentando el gasto computacional.
-   **Agrupar las edades basándote en etapas de la vida:** Este umbral corresponderá a las etapas recomendadas para tener atención al cáncer de mama según varios convenios. siendo más claro tomaremos: menos de 40 (mujeres jóvenes), 40-49 (edad en la que se recomienda comenzar las mamografías), 50-69 (edad en la que las mamografías son más efectivas), 70 y más (edad avanzada), agrupando la edad en estos 4 grupos.
-   **agrupación binaria:** En clase se habló de que en hospital es muy habitual dividir las edades en menores de 25 (jóvenes), de 25-50 (adultos) y 50 a 75 (adultos mayores), aunque como nuestros datos van de 24 a 75, decidí optar por dos subgrupos, siendo adulto joven las edades entre 24 y 49 años y un adulto mayor entre 50 y 75 años. 
```{r agrupacion_Edad}
# Agrupación de edades basada en etapas de la vida
datos_cancer$edad_umbralizada <- cut(datos_cancer$Edad,
                              breaks = c(-Inf, 40, 49, 69, Inf),
                              labels = c("(40 o menos)", "(40-49)", "(50-69)", "(70 o más)"),
                              include.lowest = TRUE)
datos_cancer$edad_binaria <- ifelse(datos_cancer$Edad < 50, "Adulto joven (24-49)", "Adulto mayor (50-75)")
head(datos_cancer, 4)
```

# **Busqueda de información previa al trabajo:**

Ya teniendo nuestros datos en orden (ya se eliminaron los errores, imputaron datos perdidos y otras medidas), antes de diseñar un modelos predictivo, es bastante útil recordar de la practica anterior el estudio de las variabeles mediante histogramas, diagramas de caja, curvas de densidad, gráficos circulares, entre otros. Que ayuden a visualizar de mejor manera la distribución de las variables. (A continuación se reciclará la función de la práctica anterior y más adelante se hablaran de los casos más interesantes que nos pueden dar información útil sobre que esperar en los resultados de modelos predictivos)

```{r Graficos y Distribuciones de frecuencia}
generador_graficas_automatico <- function(dato1, nom_col) {
 # Asegurarse de que la variable es un factor
 dato1 <- as.factor(dato1)
  
 es_cat1 <- is.factor(dato1)
  
 if (es_cat1) {
    # Gráfico de barras
    barplot(table(dato1), main = paste("Gráfico de barras de", nom_col), xlab = nom_col, ylab = "Frecuencia")
    
    # Pastel
    pie(table(dato1), cex = 1.5, main = paste("Pastel de:", nom_col))
 } else {
    # Boxplot
    boxplot(dato1, horizontal = TRUE, main = paste("Boxplot de:", nom_col))
    
    # Histograma
    hist(dato1, main = paste("Histograma de", nom_col), ylab = "Frecuencia", xlab = nom_col)
    
    # Curva de densidad
    densidad <- density(dato1)
    lines(densidad, col = "black", lwd = 2)
    
    # Tabla de frecuencias
    tabla_frecuencias <- data.frame(Valor = unique(dato1), Frecuencia = table(dato1))
    print(tabla_frecuencias)
 }
}

# Obtener los nombres de las columnas
#nombres_columnas <- colnames(datos_cancer)

# Si queremos generar los comandos tendremos que quitar el #
#resultados_analisis <- lapply(nombres_columnas, function(nombre_col) {
#  generador_graficas_automatico(datos_cancer[[nombre_col]], nombre_col)
#  View(datos_cancer)
#})
```
## **Grafica PCR**

Antes de hacer validación podemos visualizar que tan dificil será el problema dependiendo de la distribución de la variable evento, al saber si las clases estan desbalanceadas. A continuación una grafíca de la variable PCR mostrando el número de personas 1 para pacientes en metástasis y 0 para los que no, veremos que nuestras clases son desequilibradas, para esto lo primero que haríamos es buscar un modelo simple, en este programa si no tuvieramos cuidado un posible modelo es que todos los pacientes tienen 0 en la variable evento y esto nos daría casi un 80% de acierto, cuando es un modelo ridiculo que no tiene ningún sentido usar.

Una solución que podrámos evaluar en la parte de división de datos es el oversampling (sobremuestreo) que es una técnica de muestreo que se emplea habitualmente cuando tenemos una baja proporción de casos positivos en clasificaciones binomiales.

```{r grafica_PCR}
barplot(table(datos_cancer$PCR), main = "Gráfico de barras de PCR", xlab = "PCR", ylab = "Frecuencia")

#porcentaje de 0 en la variable PCR
porcentaje_0 <- round(sum(datos_cancer$PCR == "0") / length(datos_cancer$PCR) * 100, 2)
print(paste("El porcentaje de la variable PCR que tiene un valor de '0' es:", porcentaje_0, "%"))
```

## **Gráficas de barras Her2**

Otra gráfica que podríamos destacar es el gráfico de barras de Her2, la cual nos muestra el exagerado desequilibrio que tiene esta variable, y que contamos con muy pocos casos de Her2 positivo, esto la hace una variable no muy confiable inicialmente.

Más adelante se decidirá que sé hará, pero probablemente es que terminemos descartando dichca variable ya que a la hora de aplicar el stepAIC una clase con una distribución donde cuanta con tan solo 5 P es muy dificil que sea elegida como una variable importante para predecir PCR, y al momento de aplicar fuerza bruta contar con esta variable puede generar más problemas al poder distribuirse todas las P en el grupo de entrenamiento o de testeo, dejando el otro porcentaje con un solo level para evaluar.

```{r grafica_Her2}
barplot(table(datos_cancer$Her2), main = "Gráfico de barras de HER2", xlab = "Her2", ylab = "Frecuencia")
```

## **Resúmen de las edades**
En este trabajo se realizó distintos umbrales para evaluar la edad, a la hora de ver la distribución tanto en el caso no umbralizado como las 2 umbralizaciones tuvieron un buen reparto de pacientes y a primera instancia se ve que es una idea interesante evaluar los 3 casos que agrupan de manera diferente. 

## **variable Muestra**
Por ultimo podría destacar lo que se viene diciendo de la variable muestra no es una variable útil para hacer un análisis real, cada muestra tiene un valor clave identificativo que este es el valor es único para cada muestra por ende a la hora de crear un modelo podremos descartar esta variable.


# **División de datos, entrenamiento de modelos y evaluación de modelos:**

Un **modelo predictivo**, también conocido como modelo de predicción, es un conjunto de herramientas y técnicas estadísticas que se utilizan para pronosticar y predecir el comportamiento futuro basándose en datos históricos. En nuestro caso, se buscará el mejor modelo a partir de los datos que hemos estado trabajando, _logrando un modelo capaz de predecir la variable evento para nuevos pacientes._

Para este trabajo, me centraré primero en **hablar de las herramientas por separado** y al finalizar haré un análisis más exhaustivo de los resultados y las combinaciones distintas que nos pueden proporcionar. Aunque no deben tener un orden específico, se trabajará siguiendo la siguiente estructura:

**1. Dataset elegido y técnicas de procesamiento:**

Antes de crear un modelo, hay un último paso que podemos hacer y es aplicar técnicas para mejorar nuestro dataset únicamente con el objetivo de dar un mejor modelo. Aquí a partir del modelo obtenido hasta el momento y consideraremos la aplicación de técnicas como:

-   **Eliminación de variables:**  Se eliminan variables que pueden dar problemas a futuro, como es el caso de muestras y her2.

-   **Oversampling:** Se utiliza para aumentar el número de observaciones en la clase minoritaria. Buscando un equilibro en la variable.

-   **Undersampling:** Al contrario del Over se utiliza para disminuir el número de observaciones en la clase mayoritaria.

**2. Métodos de evaluación del modelo:**

Antes de trabajar en un modelo, lo ideal sería saber qué queremos y a qué nos referimos como buen modelo. Existe una gran cantidad de métodos para evaluar que nos dan distintas métricas y robustez de resultados. Se usarán métricas como:

-   **Matriz de confusión:**  Esta matriz muestra el número de casos correctamente clasificados y mal clasificados en cada categoría.

-   **Accuracy:**  Mide la proporción de casos correctamente clasificados.

-   **Sensibilidad:**   Mide la proporción de casos positivos correctamente clasificados como positivos.

-   **Especificidad:**  Mide la proporción de casos negativos correctamente clasificados como negativos.

-   **F1-score:** Mide la media armónica entre la sensibilidad y la especificidad.

-   **Curva ROC:** Esta curva muestra la relación entre la tasa de verdaderos positivos (TPR) y la tasa de falsos positivos (FPR) para diferentes umbrales de clasificación.

-   **Área bajo la curva (AUC):** Esta medida indica la capacidad del modelo para discriminar entre casos positivos y negativos.

**3. Validación interna de los modelos:**

Antes de evaluar un modelo, se tiene que saber qué tipo de validación interna se va a usar. Se pueden utilizar diversas prácticas, como:

-   **Testear la calidad del modelo con los propios datos que se usaron:** a pesar que esta práctica no es recomendable, ya que puede dar lugar a un sobreajuste del modelo. Se busca observar los resultados que puede dar.
-   **Metodología holdout con diversas proporciones:** Se divide el dataset en dos conjuntos: uno de entrenamiento y otro de prueba. El modelo se entrena con el conjunto de entrenamiento y se evalúa con el conjunto de prueba.
-   **Método de “Repeated-HoldOut”:** Práctica similar al anterior, pero con número de iteraciones que nos dará resultados más robustos al coste de un mayor esfuerzo computacional.
-   **Método de “k-fold Cross Validation:**  Se divide el dataset en k conjuntos. Se entrena el modelo k veces, utilizando cada vez un conjunto diferente como conjunto de prueba.
 
Para probar cada punto se usará tanto el modelo utilizando todas las variables como el uso del stepAIC para cada uno de los puntos anteriormente dicho. Al finalizar el análisis de las herramientas por separado, **se realizará un análisis exhaustivo** de los resultados y las combinaciones distintas que nos pueden proporcionar. Se compararán los diferentes modelos y se seleccionará el mejor modelo en función de los resultados obtenidos.

Es importante destacar que no existe un único método de evaluación perfecto. La elección del método o métodos a utilizar dependerá del problema específico que se esté abordando y de los objetivos del análisis, en este trabajo se busca aprender más que obtener una solución final apta para todo problema.


## **Dataset elegido y técnicas de procesamiento:**

A pesar de que hasta el momento llevamos un buen tiempo dedicado a la limpieza de datos y organización del dataset, todavía quedan cosas que podríamos aplicar. Ya en este punto de la practica se tomarán decisiones que favorezcan el trabajo de un modelo predictivo lo más fácil posible y que nos asegure buenos resultados.

### **Eliminación de variables problemáticas:**

El análisis previo ha identificado variables que pueden obstaculizar el rendimiento de los modelos predictivos. La variable **muestra**, al ser un identificador único para cada muestra, no aporta información relevante para la predicción y genera errores en las metodologías exploradas. Por lo tanto, se eliminará del dataset.

Adicionalmente, se propone eliminar la variable Her2 en un nuevo dataframe. Durante la división de datos en conjuntos de entrenamiento y prueba (holdout), una distribución desigual exagerada como la de Her2 podría generar errores en el análisis. Eliminar esta variable evitará este problema.

Pero en su mayoría vamos a trabajar evaluando las columnas: "Edad",  "REst", "RPro", "Estadio", "NodAfec", "Grado", "Fenotipo", *"PCR"*,  "edad_umbralizada" y "edad_binaria" 

```{r Eliminación de variables problematicas}
datos_cancer<- subset(datos_cancer, select=-Muestra) #Dataset tras limpieza e imputación
datos_cancer_def<- subset(datos_cancer, select=-Her2) #Dataset principal con el que se trabajará (sin Her2)
#print(colnames(datos_cancer_def))
```


### **Oversampling para equilibrar la distribución de clases:**

La distribución desigual de las clases en el dataset puede afectar negativamente el rendimiento del modelo. Para abordarlo, se creará un nuevo dataframe llamado datos_oversampling. En este dataframe, la clase minoritaria se sobremuestrea replicando observaciones aleatoriamente. El objetivo es equilibrar la distribución de las clases y mejorar el rendimiento del modelo.

```{r oversampling}

oversample_data <- function(dataset, target_col, event_posi=1, event_nega=0, semilla=10) {
  set.seed(semilla)
  # Dividir el conjunto de datos en aquellas cuyo valor de la variable evento es positivo (1) y negativo (0)
  positive <- dataset[dataset[[target_col]] == event_posi, ]
  negative <- dataset[dataset[[target_col]] == event_nega, ]
  
  # Calcular el número de instancias positivas y negativas
  num_positive <- nrow(positive)
  num_negative <- nrow(negative)
  
  # Determinar cuál es la clase menos representada
  minor <- ifelse(num_positive < num_negative, 1, 0)
  
  # Determinar cuántas instancias adicionales necesitamos generar
  num_to_generate <- abs(num_positive - num_negative)
  
  # Realizar oversampling generando instancias adicionales aleatorias de la clase menos representada
  if (minor == 1) {
    additional <- positive[sample(num_positive, num_to_generate, replace = TRUE), ]
  } else {
    additional <- negative[sample(num_negative, num_to_generate, replace = TRUE), ]
  }
  
  # Combinar el conjunto de datos original con las instancias adicionales
  oversampled_dataset <- rbind(dataset, additional)
  
  return(oversampled_dataset)
}
datos_oversampling= oversample_data(datos_cancer_def,"PCR")
cat("El número de filas de datos_oversampling es ", nrow(datos_oversampling), " donde la variable evento se encuentra equilibrada artificialmente.\n")

```


### **Undersampling para equilibrar la distribución de clases:**

De forma similar al oversampling, se creará un nuevo dataframe llamado datos_undersampling. En este caso, la clase mayoritaria se submuestrea eliminando observaciones aleatoriamente. El objetivo, nuevamente, es equilibrar la distribución de las clases y mejorar el rendimiento del modelo.

```{r undersample}
undersample_data <- function(dataset, target_col, event_posi=1, event_nega=0, seed=69) {
  set.seed(seed)
  # Divide the dataset into positive and negative instances
  positive <- dataset[dataset[[target_col]] == event_posi, ]
  negative <- dataset[dataset[[target_col]] == event_nega, ]

  # Calculate the number of positive and negative instances
  num_positive <- nrow(positive)
  num_negative <- nrow(negative)

  # Determine the majority class
  majority <- ifelse(num_positive > num_negative, 1, 0)

  # Determine how many instances to remove from the majority class
  num_to_remove <- abs(num_positive - num_negative)

  # Perform undersampling by randomly removing instances from the majority class
  if (majority == 1) {
    to_remove <- sample(nrow(positive), num_to_remove, replace = FALSE)
    positive <- positive[-to_remove, ]
  } else {
    to_remove <- sample(nrow(negative), num_to_remove, replace = FALSE)
    negative <- negative[-to_remove, ]
  }

  # Combine the undersampled dataset
  undersampled_dataset <- rbind(positive, negative)

  return(undersampled_dataset)
}

datos_undersampling <- undersample_data(datos_cancer_def, "PCR")
cat("El número de filas de datos_undersampling es ", nrow(datos_undersampling), " donde la variable evento se encuentra equilibrada artificialmente.\n")
```

### **Resumen los datasets trabajados:**

Hasta ahora hemos visto distintas formas de tener el dataset a continuación un breve resumen de los datasets y las ventajas y desventajas que conllevan:

-   _datos_iniciales:_ Es el dataset original, inicialmente mi intención era mostrar las pocas posibiliades o errores que nos puede dar por tratar con los datos con NA, valores erroneos y tal cual fueron dados. No obstante ni vale la pena mostrarlo en el trabajo, trabajar con datos sin ningún control de limpieza funciones como stepAIC, HoldOut, métodos de cálculo de métricas automáticas, dan muchos errores al ser suceptibles a NA, clases desequilibradas y sobrecarga de trabajo con variables, mi conclusión es que la cantidad de errores que da trabajar con los datos sin limpiar hace que ni valga la pena de ponerlo como opción final.

-   _datos_cancer:_ es la versión limpia del dataset original, sin la variable muestra y conservando Her2. Esta conservación puede ser beneficiosa al ser más fiel a los datos originales, pero la división aleatoria en entrenamiento y test puede generar errores debido al desequilibrio entre las clases. Para evitarlo, se podría utilizar técnicas de muestreo estratificado (conocidos).

-   _datos_cancer_def:_ Es parecida a la anterior pero tomando la libertad de descartar Her2, este modelo es muy parecido a los datos iniciales pero nos facilita muchisimo el uso de los futuros métodos, contando con el único problema de que la variable evento esta desequilibrada (cerca del 20% es evento positivo y el 80% es negativo).

-   _datos_oversampling:_ Sé le aplica oversampling a datos_cancer_def y ahora podremos trabajar con este dataset muy facilmente al tener la variable evento equilibrada, el problema es que tiene datos ficticios generando casi 290 filas donde la variable evento sea positiva. Esto ya hace que los resultados no sean muy fiables.

-   _datos_undersampling:_ En vez de over se le aplica under en vez de generar 290 filas donde la variable evento sea positiva, esta vez descartamos 290 filas, aunque esta ocasión no tenemos datos ficticios, reducimos el total de filas a 198, quedando un dataset que esta equilibrado y es fiable, los datos de entrenamiento y de testeo serán mucho menor y más susceptibles  a que los datos erroóneos tomen más peso.


## **stepAIC** 

Antes de continuar voy a llamar la función StepAIC para tener un modelo bueno al cúal evaluar. *StepAIC* es una herramienta útil para la selección de modelos en análisis de regresión. Ayuda a encontrar el equilibrio entre la complejidad del modelo y su capacidad para explicar los datos observados. En el contexto del dataset utilizado, StepAIC identificó las siguientes variables como las más importantes para predecir _PCR:_ **Estadio**, **Grado** y **Fenotipo**. Es importante tener en cuenta que aunque StepAIC ofrece una buena calidad en la selección de variables, no obstante, no garantiza que la combinación identificada sea la óptima. Esto se debe a que StepAIC no evalúa exhaustivamente todas las combinaciones posibles de variables. Para abordar esta limitación, más adelante aplicaremos un algoritmo de fuerza bruta que nos proporcionará una tabla con todas las combinaciones posibles, lo que nos permitirá validar si la solución propuesta por StepAIC sigue siendo la mejor opción.

Además, es interesante destacar la umbralización de las edades, en la edad binaria que consta en la división en dos grupos, donde los adultos jóvenes tienen entre 24 y 50 años, y los adultos mayores tienen entre 51 y 75 años. Esta variable inicialmente fue eliminada sin afectar el AIC, lo que sugiere que este agrupamiento puede ser demasiado general para el dataset y el caso específico en cuestión. Por otro lado, la umbralización de la edad en cuatro grupos específicos para el cáncer de mama resultó ser efectiva. Redujimos 49 edades distintas a solo 4 grupos y observamos que la edad umbralizada, que abarca desde los 40 hasta los 49 años, es el segundo coeficiente más relacionado con PCR. Además, es importante destacar que tanto la edad como la edad umbralizada fueron las últimas variables en ser eliminadas durante el proceso de selección de características.
```{r stepAIC}
# Hay que cargar la librería MASS, donde se encuentra la función stepAIC
library(MASS)
# Stepwise backward selection
# ---------------------------
# Primero, se crea el modelo completo. El "." indica que incluye todas las variables del conjunto de datos.
# En el parámetro data se pasan los datos imputados mediante la eliminación de los NA´s. Es importante destacar que el algoritmo stepAIC no funciona correctamente con valores NA´s.
lr.fit1 <- glm(PCR ~ ., data = datos_cancer_def, family = binomial("logit"))
summary(lr.fit1)
# Realizar selección de variables utilizando stepAIC
modelo_seleccionado <- stepAIC(lr.fit1, direction = "backward")
# Resumen del modelo seleccionado
summary(modelo_seleccionado)
```
Las variables más importantes son grado, estadio, fenotipo.
De entre todas ellas fenotipo es la más importante teniendo los valores de femLumA y B como los mas relevantes en cuanto a relacion con PCR refiere.
Objetivos practica 2 Mineria de Datos:

https://www.juanbarrios.com/la-matriz-de-confusion-y-sus-metricas/
2. Probar meter 100% de los datos en el modelo
```{r Calcular_parametro_matriz}
# Realizar predicciones en el conjunto de prueba
predicciones <- predict(lr.fit1, newdata = datos_cancer_def, type = "response")

# Evaluar el rendimiento del modelo
predicted_classes <- ifelse(predicciones > 0.5, 1, 0)
conf_matrix <- table(datos_cancer_def$PCR, predicted_classes)

print("La matriz confusión:")
print(conf_matrix)

calcular_param_val_INCOMPLETA <- function(matriz_confusion) {
    # Comprobar si la matriz de confusión es 2x1
  if (ncol(matriz_confusion) == 1 && nrow(matriz_confusion) == 2) {
    # Crear una matriz de confusión 2x2 rellenada con ceros
    new_conf_matrix <- matrix(0, nrow = 2, ncol = 2, dimnames = list(c("0", "1"), c("0", "1")))
    # Copiar los valores de la matriz original a la nueva matriz
    new_conf_matrix[, 1] <- matriz_confusion[, 1]
    matriz_confusion <- new_conf_matrix
  }
  
  VN <- matriz_confusion[1, 1]
  FP <- matriz_confusion[1, 2]
  VP <- matriz_confusion[2, 2]
  FN <- matriz_confusion[2, 1]
  #La Exactitud  (en inglés, “Accuracy”) (VP+VN)/(VP+FP+FN+VN) ACIERTOS/TOTAL
  accuracy <- round((VP+VN)/(VP+FP+FN+VN), digits = 2)

  # La Precisión VP/(VP+FP), lo que sería igual :  Verdaderos positivos / Total Enfermos
  precision <- round(VP/(VP+FP), digits = 2)
  
  # La Sensibilidad (“Recall” o “Sensitivity” ),VP/(VP+FN)
  sensibilidad <- round((VP/(VP+FN)), digits = 2)


  # La Especificidad (“Especificity”) también conocida como la Tasa de Verdaderos Negativos, (“true negative rate”)
  #VN/(VN+FP)
  especificidad <- round((VN/(VN+FP)), digits = 2)

  # Calcular el F1-score
  f1_score <- round(2 * (precision * sensibilidad) / (precision + sensibilidad), digits = 2)
  
  return(c(accuracy, sensibilidad, especificidad, f1_score, precision))
}

imprimir_param_eval2 <- function(calc){
    cat("Parámetros de evaluación del modelo:",
      paste("Accuracy:", calc[1]),
      paste("precision:", calc[5]),
      paste("Sensibilidad:", calc[2]),
      paste("Especificidad:", calc[3]),
      paste("F1-score:", calc[4]), 
      "\n")
  
}


# Obtener los valores
calc <- calcular_param_val_INCOMPLETA(conf_matrix)

imprimir_param_eval(calc)

```


Función que calcula AUC

```{r, message=FALSE}

library(pROC)
calcular_param_val <- function(verdadescomopuños, prediccionesdelseñor) {
  
  suppressWarnings(matriz_confusion <- table(verdadescomopuños, prediccionesdelseñor))
    # Comprobar si la matriz de confusión es 2x1
  if (ncol(matriz_confusion) == 1 && nrow(matriz_confusion) == 2) {
    # Crear una matriz de confusión 2x2 rellenada con ceros
    new_conf_matrix <- matrix(0, nrow = 2, ncol = 2, dimnames = list(c("0", "1"), c("0", "1")))
    # Copiar los valores de la matriz original a la nueva matriz
    new_conf_matrix[, 1] <- matriz_confusion[, 1]
    matriz_confusion <- new_conf_matrix
  }
  
  #Precision: The proportion of positive predictions that are correct. ACC = TP + TN / (TP + FP + FN + TN)
  accuracy <- round(sum(diag(matriz_confusion)) / sum(matriz_confusion), digits = 2)

  # TP/(TP+FN),   o lo que sería igual :
  recall <- round(matriz_confusion[2, 2] / sum(matriz_confusion[2, ]), digits = 2)
  
  #precision=TP/TP+FP
  precision <- round(matriz_confusion[2, 2] / sum(matriz_confusion[, 2]), digits = 2)


  # En términos de salud:  TN/TN+FP
  specificity <- round(matriz_confusion[1, 1] / sum(matriz_confusion[1, ]), digits = 2)
  
  
  # Calcular el F1-score
  f1_score <- round(2 * (precision * recall) / (precision + recall), digits = 2)

  #te calculo la roc
  roc_obj <- roc(verdadescomopuños, prediccionesdelseñor)

  # Valor del área bajo la curva (AUC)
  auc <- round(auc(roc_obj), digits = 3)
  
  return(c(accuracy, recall, specificity, f1_score, precision,auc))
}

imprimir_param_eval <- function(calc){
    cat("Parámetros de evaluación del modelo:",
      paste("Accuracy:", calc[1]),
      paste("Precision:", calc[5]),
      paste("Recall:", calc[2]),
      paste("Specificity:", calc[3]),
      paste("F1-score:", calc[4]), 
      paste("Área bajo la curva ROC (AUC):", calc[6]),
      "\n")
  
}


# Obtener los valores
calc <- calcular_param_val(datos_cancer_def$PCR, predicted_classes)

imprimir_param_eval(calc)
```

## **división de datos:**

Para crear los modelos tenemos varias opciones desde manipulación del dataset como vendría a ser usar el dataset sin datos imputados, con datos imputados, con oversampling, con undersampling, etc. como también la distribución de entrenamiento validación y testeo:
- 
Validación cruzada:
también recomendó probar distintas cantidades de división (70/30, 80/20 o 60/40)
dividir entre entrenamiento, validación y testeo
Oversampling
modelo imputando datos, modelo sin imputar, modelo con variables a lo bruto, modelo oversampling, modelo undersampling y ya el repeated hold out

```{r division de datos}
dividir_datos <- function(datos, porcentaje = 70, semilla = 42) {
 # Establecer la semilla para la reproducibilidad
 set.seed(semilla)
  
 # Calcular el número de filas para el conjunto de entrenamiento
 n_entrenamiento <- floor(porcentaje / 100 * nrow(datos))
  
 # Crear índices para el conjunto de entrenamiento
 indices_entrenamiento <- sample(1:nrow(datos), size = n_entrenamiento)
  
  
 return(indices_entrenamiento)
}
indices=dividir_datos(datos_cancer,70)
#comparable con set.seed(123) # Para reproducibilidad
#indices <- createDataPartition(datos_cancer_def$PCR, p = 0.8, list = FALSE)
```

```{r, warning=FALSE, message=FALSE}
# Cargar la biblioteca necesaria
library(MASS)
library(caret)
# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(42) # Para reproducibilidad
indices <- createDataPartition(datos_cancer_def$PCR, p = 0.7, list = FALSE)
datos_entrenamiento <- datos_cancer_def[indices, ]
datos_prueba <- datos_cancer_def[-indices, ]

# Entrenar el modelo de regresión logística sin la columna Muestra
modelo <- glm(PCR ~ ., data = datos_entrenamiento, family = "binomial")

# Realizar predicciones en el conjunto de prueba
predicciones <- predict(modelo, newdata = datos_prueba, type = "response")
predicted_classes2 <- ifelse(predicciones > 0.5, 1, 0)

calc2 <- calcular_param_val(datos_prueba$PCR, predicted_classes2)
imprimir_param_eval(calc2)
```


## curva roc para todos los datos
```{r curva roc, message=FALSE}

library(pROC)


# Calcular la curva ROC
roc_obj <- roc(datos_prueba$PCR, predicciones)

# Graficar la curva ROC
plot(roc_obj, main='Curva ROC', print.auc=TRUE, auc.polygon=TRUE,
     grid=TRUE, legacy.axes=TRUE, percent=TRUE)

# Imprimir el valor del área bajo la curva (AUC)
cat('Área bajo la curva (AUC):', auc(roc_obj), '\n')

```

# repeted holdout

```{r holdOut, message=FALSE}
library(MASS)
library(caret)


# Definir la función HoldOut
HoldOut <- function(datos, num_repeticiones = 1, semilla = 10, mostrar_resultado = TRUE) {
  set.seed(semilla)
  proporciones <- c(0.6, 0.7, 0.8)
  mejor_proporcion <- 0
  mejor_AUC <- 0
  mejores_parametros <- NULL
  
  for (proporcion_entrenamiento in proporciones) {
    parametros_mejor_AUC <- NULL
    metrics <- NULL
    
    for (i in 1:num_repeticiones) {
      # Dividir los datos en conjuntos de entrenamiento y prueba
      indices <- createDataPartition(datos$PCR, p = proporcion_entrenamiento, list = FALSE)
      datos_entrenamiento <- datos[indices, ]
      datos_prueba <- datos[-indices, ]
      
      # Entrenar el modelo de regresión logística sin la columna Muestra
      modelo <- glm(PCR ~ ., data = datos_entrenamiento, family = "binomial")
      
      # Realizar predicciones en el conjunto de prueba
      predicciones <- predict(modelo, newdata = datos_prueba, type = "response")
      predicted_classes <- ifelse(predicciones > 0.5, 1, 0)
      
      # Calcular la matriz de confusión
      current_metrics <- calcular_param_val(datos_prueba$PCR, predicted_classes)
      # Calcular las métricas
      # <- calcular_param_roc(conf_matrix)

      # Add current fold metrics to the overall metrics
      if (is.null(metrics)) {
        metrics <- current_metrics
      } else {
        metrics <- metrics + current_metrics
        }
    }
    
    # Calcular la precisión promedio para esta proporción
    AUC_promedio <- metrics[6] / num_repeticiones
    # Actualizar la mejor proporción si se encuentra una precisión mejor
    if (AUC_promedio > mejor_AUC) {
      mejor_AUC <- AUC_promedio
      mejor_proporcion <- proporcion_entrenamiento
      mejores_parametros <- metrics/num_repeticiones
    }
  }
  
   if (mostrar_resultado) {
    cat("La mejor proporción es", mejor_proporcion, "con un AUC promedio de", mejor_AUC, "\n")
  }
  mejores_parametros <- round(mejores_parametros, digits = 2)
  # Devolver la mejor precisión y los demás parámetros para esa precisión
  return(mejores_parametros)
}

parametros <- HoldOut(datos_oversampling,num_repeticiones = 1,semilla = 10,mostrar_resultado = TRUE)
imprimir_param_eval(parametros)
parametros <- HoldOut(datos_cancer_def,num_repeticiones = 3,semilla = 10,mostrar_resultado = TRUE)
imprimir_param_eval(parametros)
```

KFOLDCROSSVALIDATION :D
```{r KFOLD DEFINITIVO MAMADISIMO, message=FALSE}
k_fold_cross_validation <- function(data, k = 10, semilla=50) {
  # Shuffle the data
  set.seed(semilla)
  shuffled_data <- data[sample(nrow(data)), ]
  
  # Calculate the number of rows in each fold
  fold_size <- floor(nrow(data) / k)
  
  metrics <- NULL
  
  for (i in 1:k) {
    # Define the start and end indices for the current fold
    start_index <- ((i - 1) * fold_size) + 1
    end_index <- min(i * fold_size, nrow(data))
    
    # Extract the current fold for validation
    validation_data <- shuffled_data[start_index:end_index, ]
    
    # Use the rest of the data for training
    training_data <- rbind(shuffled_data[1:start_index-1, ], shuffled_data[end_index+1:nrow(shuffled_data), ])
    
    # Train the model on the training data (using default model)
    model <- glm(PCR ~ ., data = training_data, family = "binomial")
    
    # Make predictions on the validation set
    predicted_classes <- ifelse(predict(model, newdata = validation_data, type = "response") > 0.5, 1, 0)
    
    # Calculate metrics
    current_metrics <- calcular_param_val(validation_data$PCR, predicted_classes)
    
    # Add current fold metrics to the overall metrics
    if (is.null(metrics)) {
      metrics <- current_metrics
    } else {
      metrics <- metrics + current_metrics
    }
  }
  
  # Calculate mean of metrics
  metrics_mean <- metrics / k
  
  # Return the mean metrics
  return(metrics_mean)
}

calc <- k_fold_cross_validation(datos_cancer_def)
imprimir_param_eval(calc)
calc <- k_fold_cross_validation(datos_oversampling)
imprimir_param_eval(calc)
```
#fuerza bruta javi intento final

```{r, message=FALSE}
buscar_todos_modelos <- function(datos, variable_evento, num_rep=0, folds=0) {
  # Obtener todas las combinaciones de variables predictoras
  variables <- colnames(datos)
  variables <- variables[variables != variable_evento]  # Excluir la variable objetivo
  combinaciones <- lapply(1:length(variables), function(n) combn(variables, n))

  # Inicializar una lista para almacenar los resultados
  resultados <- list()

  # Iterar sobre todas las combinaciones
  for (i in 1:length(combinaciones)) {
    for (j in 1:ncol(combinaciones[[i]])) {
      predictoras <- c(combinaciones[[i]][, j], variable_evento)
      datos_filtrados <- datos[, predictoras]
      
        if (num_rep != 0) {
         # Llamar a la función HoldOut con num_repeticiones=num_rep
          parametros <- HoldOut(datos_filtrados, num_repeticiones = num_rep, mostrar_resultado = FALSE)
        } else if (folds != 0) {
         # Llamar a la función k_fold_cross_validation con k=k
           parametros <- k_fold_cross_validation(data = datos_filtrados, k = folds)
        } else {
         # Salir y finalizar la ejecución de la función
          stop("Los parámetros de entrada no son correctos.")
        }

      
      accuracy <- parametros[1] 
      precision <- parametros[5]
      sensibilidad <- parametros[2] 
      especificidad <- parametros[3]
      f1_score <- parametros[4] 
       AUC <- parametros[6]

      
      # Guardar los resultados en la lista
      resultados[[length(resultados) + 1]] <- c(paste(predictoras[-length(predictoras)], collapse = ", "), accuracy, precision, sensibilidad, especificidad, f1_score, AUC)
    }
  }
  
  # Convertir la lista de resultados en un dataframe
  resultados_df <- do.call(rbind, resultados)
  colnames(resultados_df) <- c("Variables",  "Accuracy", "Precision", "Sensibilidad","Especificidad", "F1_score", "AUC")
  
  return(resultados_df)
}


# Ejemplo de uso
resultados_tabla_oversampling <- buscar_todos_modelos(datos = datos_oversampling, variable_evento = "PCR", folds = 5)
View(resultados_tabla_oversampling)
```



# Fuerza bruta de todos los modelos:
```{r FUERZABRUTA, message=FALSE}
buscar_todos_modelos <- function(datos, variable_evento, num) {
  # Obtener todas las combinaciones de variables predictoras
  variables <- colnames(datos)
  variables <- variables[variables != variable_evento]  # Excluir la variable objetivo
  combinaciones <- lapply(1:length(variables), function(n) combn(variables, n))

  # Inicializar una lista para almacenar los resultados
  resultados <- list()

  # Iterar sobre todas las combinaciones
  for (i in 1:length(combinaciones)) {
    for (j in 1:ncol(combinaciones[[i]])) {
      predictoras <- c(combinaciones[[i]][, j], variable_evento)
      datos_filtrados <- datos[, predictoras]
      

      parametros <- HoldOut(datos_filtrados,  1, 10, mostrar_resultado = FALSE)
      
      accuracy <- parametros[1] 
      precision <- parametros[5]
      sensibilidad <- parametros[2] 
      especificidad <- parametros[3]
      f1_score <- parametros[4] 
      AUC <- parametros[6]
      

      
      # Guardar los resultados en la lista
      resultados[[length(resultados) + 1]] <- c(paste(predictoras[-length(predictoras)], collapse = ", "), accuracy, precision, sensibilidad, especificidad, f1_score, AUC)
    }
  }
  
  # Convertir la lista de resultados en un dataframe
  resultados_df <- do.call(rbind, resultados)
  colnames(resultados_df) <- c("Variables", "Accuracy", "Precision", "Sensibilidad","Especificidad", "F1_score", "AUC")
  
  return(resultados_df)
}


# Ejemplo de uso
resultados_tabla_oversampling <- buscar_todos_modelos(datos = datos_oversampling, variable_evento = "PCR")
View(resultados_tabla_oversampling)

```

PORSIACA SOLO ACCURACCY PARA 
```{r}
buscar_todos_modelos <- function(datos, variable_evento) {
  # Obtener todas las combinaciones de variables predictoras
  variables <- colnames(datos)
  variables <- variables[variables != variable_evento]  # Excluir la variable objetivo
  combinaciones <- lapply(1:length(variables), function(n) combn(variables, n))

  # Inicializar una lista para almacenar los resultados
  resultados <- list()

  # Iterar sobre todas las combinaciones
  for (i in 1:length(combinaciones)) {
    for (j in 1:ncol(combinaciones[[i]])) {
      predictoras <- c(combinaciones[[i]][, j], variable_evento)
      datos_filtrados <- datos[, predictoras]
      
      # Dividir los datos en entrenamiento y prueba
      set.seed(42)
      indices <- createDataPartition(datos_filtrados[[variable_evento]], p = 0.7, list = FALSE)
      datos_entrenamiento <- datos_filtrados[indices, ]
      datos_prueba <- datos_filtrados[-indices, ]
      
      # Entrenar el modelo
      modelo <- glm(paste(variable_evento, "~ ."), data = datos_entrenamiento, family = "binomial")
      
      # Realizar predicciones en el conjunto de prueba
      predicciones <- predict(modelo, newdata = datos_prueba, type = "response")
      predicted_classes <- ifelse(predicciones > 0.5, 1, 0)
      
      # Calcular la precisión
      conf_matrix <- table(datos_prueba[[variable_evento]], predicted_classes)
      accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
      
      # Guardar los resultados en la lista
      resultados[[length(resultados) + 1]] <- c(paste(predictoras[-length(predictoras)], collapse = ", "), accuracy)
    }
  }
  
  # Convertir la lista de resultados en un dataframe
  resultados_df <- do.call(rbind, resultados)
  colnames(resultados_df) <- c("Variables", "Accuracy")
  
  return(resultados_df)
}


# Ejemplo de uso
resultados_tabla <- buscar_todos_modelos(datos = datos_oversampling, variable_evento = "PCR")
View(resultados_tabla)

```
RESULTADOS:

Prediciendo el 100% del modelo:
	-Sin selección de variables: (métricas de val)
	-Sin selección de variables + Oversampling: (métricas de val)
	-StepAIC: (métricas de val)
	-StepAIC + Oversampling: (métricas de val)
	-Búsqueda exhaustiva: (métricas de val)
	-Búsqueda exhaustiva + Oversampling: (métricas de val)

Holdout(utilizando la mejor proporción obtenida):
	-Sin selección de variables: (métricas de val)
	-Sin selección de variables + Oversampling: (métricas de val)
	-StepAIC: (métricas de val)
	-StepAIC + Oversampling: (métricas de val)
	-Búsqueda exhaustiva: (métricas de val)
	-Búsqueda exhaustiva + Oversampling: (métricas de val)

Repeated-Holdout(utilizando la mejor proporción obtenida):
	-Sin selección de variables: (métricas de val)
	-Sin selección de variables + Oversampling: (métricas de val)
	-StepAIC: (métricas de val)
	-StepAIC + Oversampling: (métricas de val)
	-Búsqueda exhaustiva: (métricas de val)
	-Búsqueda exhaustiva + Oversampling: (métricas de val)

k-fold Cross Validation(utilizando el mejor valor de k obtenido):
	-Sin selección de variables: (métricas de val)
	-Sin selección de variables + Oversampling: (métricas de val)
	-StepAIC: (métricas de val)
	-StepAIC + Oversampling: (métricas de val)
	-Búsqueda exhaustiva: (métricas de val)
	-Búsqueda exhaustiva + Oversampling: (métricas de val)

*Trabajo de míneria de datos:* (entiendo que esto es medio todo lo que menciono durante clases y que implícitamente es lo que espera del trabajo) 
1) *Lectura y corrección de datos* (eliminar los NA de PCR, imputar datos perdidos, maybe agrupar las edades y otros cambios/modificaciones que veamos necesarias)
2) *División de datos* (Oversampling (obligatorio dicho en clase?, la división es en entrenamiento, validación y prueba, también recomendó probar distintas cantidades de división (70/30, 80/20 o 60/40), Dijo también probar comparar resultados con el dataset con limpieza y datos imputados contra dataset donde se borraron los  datos perdidos?)
3) *METRICAS DE RENDIMIENTO* (Calcular el ACC, sensibilidad (o recall), especificidad y f1-score  para el modelo de regresión logística obtenido en la actividad #1. Y elegir el mejor modelo de los elegidos en el punto 2?)
4) *Validación interna del modelos* (calcular holdout, repetir varias veces para confirmar estabilidad de resultados (recomendó 30 veces para tener un umbral de confianza dando la media y la varianza del accuracy obtenido), obtener una curva roc dado por promedio de AOC
3.? probar Método de “Repeated-HoldOut” y Método de “k-fold Cross Validation” (aún no he llegado aqui por estar corrigiendo cosas que me salte)

¿Por qué es necesario hacer validación?  En este caso al no tener “Case fall” no sirve para nada, pero el profe da la conclusión de que necesita validación porque al obtener resultados pued ser que no se pueda seguir con los datos

# NOTA, probar generar el modelo predictivo con las filas eliminadas y con las filas modificadas? 

#Combn importante busqueda exhaustiva es la tendencia natural de las coasm que ocurre a veces es jodido usarlo al tener problemas reales (Mucha información )

# createDataPartition
# PCR
# caret

los testeos que recomienda probar con 70/30, si fueran muchos datos 80/20 si fueran pocos, y tambien cuanto hay menos o la relacion que tengan los datos con el variable 60/40 y veria el accuraccy de cada uno para ver cual es más estable, 
breve analisis descriptivo
comenzar con holdout
Despúes de obtener un holdout podriamos probarlo calcular varias veces para confirmar estabilidad, el recomendaría probar 30 veces para tener un mayor umbral de confianza con la media y la varianza del accuraccy 
Probar el aumento de datos de validación interna

Lo mejor es dar una curva roc dado por promedio de AOC, otra forma es imprimir varias curvas ROC y hacer promedio, pero en nuestro caso es  más sencillo es copiar y pegar todos los test que hemos hecho, y de esto hacer la curva ROC
